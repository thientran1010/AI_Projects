# ğŸš€ Lunar Lander â€“ Deep Q-Learning Agent

An implementation of a Deep Q-Network (DQN) agent that learns to land a spacecraft autonomously in the OpenAI Gym `LunarLander-v2` environment. The project uses reinforcement learning techniques to train the agent through trial and error, optimizing for soft landings and fuel efficiency.

---

## ğŸ“š Table of Contents

- [About the Project](#about-the-project)
- [Architecture Overview](#architecture-overview)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Contributing](#contributing)

---

## ğŸ§  About the Project

This project applies Deep Reinforcement Learning to solve the Lunar Lander problem using the DQN algorithm. The agent learns to control the spacecraft by receiving rewards and penalties based on its actions. Over time, it learns an optimal policy for successful landings.

The project was completed as part of a university software engineering course, with team members collaborating on architecture, training, evaluation, and user experience.

---

## ğŸ› ï¸ Architecture Overview

- **Environment**: LunarLander-v2 (OpenAI Gym)
- **Algorithm**: Deep Q-Learning (DQN)
- **Libraries**:
  - `PyTorch` â€“ Neural network & training
  - `NumPy` â€“ Math and matrix operations
  - `Matplotlib` â€“ Reward visualization
  - `Gym` â€“ RL environment

**Key Components**:
- `DQNAgent` â€“ Core agent logic (Q-learning)
- `ReplayMemory` â€“ Experience replay buffer
- `QNetwork` â€“ Neural net approximator
- `Trainer` â€“ Training loop for the agent
- `Visualizer` â€“ Episode performance tracking

---

## âš™ï¸ Installation

> Python 3.8 or higher is required.

1. **Clone the repo**:
```bash
cd $PATH_TO_PROJECT

#Highly Suggest that a fork to the Git Repository is created and you use the forked linked to clone the project.
git clone $FORKED_REPO_URI

cd lunar_lander_rl

git remote add upstream https://github.com/thientran1010/AI_Projects
git fetch upstream
git pull upstream main
```

2. **Install Dependencies**:
```bash
cd $WORKSPACE

pip install -r requirements.txt
```

*If you encounter issues with box2d-py, install SWIG and ensure itâ€™s in your PATH, or use Gym 0.21.0 for easier compatibility.*

---

## ğŸš€ Usage

### ğŸ§ª To train the agent:
```bash
cd $WORKSPACE/lunar_lander

python train.py
```

### ğŸ® To test a trained agent:
```bash
cd $WORKSPACE/lunar_lander

python lunar_test.py
```

### ğŸ“Š To view training progress:

Reward plots are saved under /plots after each training session.

---

## ğŸ—‚ Project Structure

<!-- TODO: Maybe rethink strucutre to look like this or change this to mimic the existing structure -->

```bash
lunar_lander_rl/
â””â”€â”€ lunar_lander/
    â”œâ”€â”€ agents/             # Agent, replay memory, networks
    â”œâ”€â”€ env/                # Environment and wrappers
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ train.py
    â”‚   â””â”€â”€ test.py
    â”œâ”€â”€ models/             # Saved models
    â”œâ”€â”€ plots/              # Reward and loss visualizations
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ README.md
    â””â”€â”€ requirements.txt
```

---

ğŸ¤ Contributing

This project was built by a student team and is open to further development. If you'd like to contribute:

1. Fork the repository
2. Create your feature branch (git checkout -b feature/YourFeature)
3. Commit your changes (git commit -m "Add feature")
4. Push to the branch (git push origin feature/YourFeature)
5. Open a Pull Request

---

âœ¨ Acknowledgements

- OpenAI Gym Team
- PyTorch Contributors




## ğŸ†• New Features

### Live Monitoring Dashboard
- Real-time episode tracking and performance metrics
- Visual demo with live data export to JSON
- Web dashboard for monitoring training progress

**Usage:**
```bash
# Terminal 1: Start web server
python3 -m http.server 8080

# Terminal 2: Run visual demo  
python3 visual_demo.py

# Browser: Open dashboard
http://localhost:8080/dashboard.html